---
title: "python_test"
author: "me"
date: "2/6/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(reticulate)
library(tensorflow)
library(tidyverse)
```

```{r setup_python}
Sys.setenv(TF_KERAS=1) 
# make sure we use python 3
reticulate::use_python('C:/Users/mattm/anaconda3/python.exe',
                       required=T)
# to see python version
reticulate::py_config()
```

```{r check_versions}
reticulate::py_module_available('keras_bert')

tensorflow::tf_version()

```


```{r initialize_training_data}
pretrained_path = 'C:/Users/mattm/OneDrive/Desktop/Gov52/machine_learning/uncased_L-12_H-768_A-12'
config_path = file.path(pretrained_path, 'bert_config.json')
checkpoint_path = file.path(pretrained_path, 'bert_model.ckpt')
vocab_path = file.path(pretrained_path, 'vocab.txt')

k_bert = import('keras_bert')
token_dict = k_bert$load_vocabulary(vocab_path)
tokenizer = k_bert$Tokenizer(token_dict)
```

```{r setup_model}
seq_length = 50L
bch_size = 70
epochs = 1
learning_rate = 1e-4

DATA_COLUMN = 'comment_text'
LABEL_COLUMN = 'target'

model = k_bert$load_trained_model_from_checkpoint(
  config_path,
  checkpoint_path,
  training=T,
  trainable=T,
  seq_len=seq_length)


```

```{r tokenizer}
# tokenize text
tokenize_fun = function(dataset) {
  c(indices, target, segments) %<-% list(list(),list(),list())
  for ( i in 1:nrow(dataset)) {
    c(indices_tok, segments_tok) %<-% tokenizer$encode(dataset[[DATA_COLUMN]][i], 
                                                       max_len=seq_length)
    indices = indices %>% append(list(as.matrix(indices_tok)))
    target = target %>% append(dataset[[LABEL_COLUMN]][i])
    segments = segments %>% append(list(as.matrix(segments_tok)))
  }
  return(list(indices,segments, target))
}
# read data
dt_data = function(dir, rows_to_read){
  data = data.table::fread(dir, nrows=rows_to_read)
  c(x_train, x_segment, y_train) %<-% tokenize_fun(data)
  return(list(x_train, x_segment, y_train))
}
```

```{r load_input_data}
c(x_train,x_segment, y_train) %<-% 
dt_data('C:/Users/mattm/OneDrive/Desktop/Gov52/machine_learning/train.csv',2000)

```

```{r list_input_data}
train = do.call(cbind,x_train) %>% t()
segments = do.call(cbind,x_segment) %>% t()
targets = do.call(cbind,y_train) %>% t()

concat = c(list(train),list(segments))

```

```{r delay_warmup}
c(decay_steps, warmup_steps) %<-% k_bert$calc_train_steps(
  targets %>% length(),
  batch_size=bch_size,
  epochs=epochs
)
```

```{r setup_in_out_layers}
input_1 = get_layer(model,name = 'Input-Token')$input
input_2 = get_layer(model,name = 'Input-Segment')$input
inputs = list(input_1,input_2)

dense = get_layer(model,name = 'NSP-Dense')$output

outputs = dense %>% layer_dense(units=1L, activation='sigmoid',
                         kernel_initializer=initializer_truncated_normal(stddev = 0.02),
                         name = 'output')

model = keras_model(inputs = inputs,outputs = outputs)
```



```{r}
model %>% compile(
  k_bert$AdamWarmup(decay_steps=decay_steps, 
                    warmup_steps=warmup_steps, learning_rate=learning_rate),
  loss = 'binary_crossentropy',
  metrics = 'accuracy'
)

```

```{r}
fit1 <- model %>% fit(
  concat,
  targets,
  epochs=epochs,
  batch_size=bch_size, validation_split=0.2)
```


```{r}
view(x_train)

names(y_train) <- c('id','target','comment_text','severe_toxicity','obscene','identity_attack','insult','threat','asian','atheist','bisexual','black','buddhist','christian','female','heterosexual','hindu','homosexual_gay_or_lesbian','intellectual_or_learning_disability','jewish','latino','male','muslim','other_disability','other_gender','other_race_or_ethnicity','other_religion','other_sexual_orientation','physical_disability','psychiatric_or_mental_illness','transgender','white','created_date','publication_id','parent_id','article_id',"rating","funny","wow","sad","likes","disagree","sexual_explicit",  "identity_annotator_count","toxicity_annotator_count")

view(y_train)

```

```{r}

model %>% fit(
  concat[1],
  targets,
  epochs = epochs,
  batch_size=bch_size, validation_split=0.2)


```


