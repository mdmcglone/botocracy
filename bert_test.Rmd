---
title: "python_test"
author: "me"
date: "2/6/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(keras)
library(reticulate)
library(tensorflow)
library(tidyverse)
```

```{r setup_python}
Sys.setenv(TF_KERAS=1) 
# make sure we use python 3
reticulate::use_python('C:/Users/mattm/anaconda3/python.exe',
                       required=T)
# to see python version
reticulate::py_config()
```

```{r check_versions}
reticulate::py_module_available('keras_bert')

tensorflow::tf_version()

```


```{r initialize_training_data}
pretrained_path = 'C:/Users/mattm/OneDrive/Desktop/Gov52/machine_learning/uncased_L-12_H-768_A-12'
config_path = file.path(pretrained_path, 'bert_config.json')
checkpoint_path = file.path(pretrained_path, 'bert_model.ckpt')
vocab_path = file.path(pretrained_path, 'vocab.txt')

k_bert = import('keras_bert')
token_dict = k_bert$load_vocabulary(vocab_path)
tokenizer = k_bert$Tokenizer(token_dict)
```

```{r setup_model}
seq_length = 50L
bch_size = 70
epochs = 1
learning_rate = 1e-4

DATA_COLUMN = 'comment_text'
LABEL_COLUMN = 'target'

model = k_bert$load_trained_model_from_checkpoint(
  config_path,
  checkpoint_path,
  training=T,
  trainable=T,
  seq_len=seq_length)


```

```{r tokenizer}
# tokenize text
tokenize_fun = function(dataset) {
  c(indices, target, segments) %<-% list(list(),list(),list())
  for ( i in 1:nrow(dataset)) {
    c(indices_tok, segments_tok) %<-% tokenizer$encode(dataset[[DATA_COLUMN]][i], 
                                                       max_len=seq_length)
    indices = indices %>% append(list(as.matrix(indices_tok)))
    target = target %>% append(dataset[[LABEL_COLUMN]][i])
    segments = segments %>% append(list(as.matrix(segments_tok)))
  }
  return(list(indices,segments, target))
}
# read data
dt_data = function(dir, rows_to_read){
  data = data.table::fread(dir, nrows=rows_to_read)
  c(x_train, x_segment, y_train) %<-% tokenize_fun(data)
  return(list(x_train, x_segment, y_train))
}
```

```{r load_input_data}
c(x_train,x_segment, y_train) %<-% 
dt_data('C:/Users/mattm/OneDrive/Desktop/Gov52/machine_learning/df2.csv',200)


train = do.call(cbind,x_train) %>% t()
segments = do.call(cbind,x_segment) %>% t()
targets = do.call(cbind,y_train) %>% t()

concat = c(list(train),list(segments))

c(decay_steps, warmup_steps) %<-% k_bert$calc_train_steps(
  targets %>% length(),
  batch_size=bch_size,
  epochs=epochs
)

input_1 = get_layer(model,name = 'Input-Token')$input
input_2 = get_layer(model,name = 'Input-Segment')$input
inputs = list(input_1,input_2)

dense = get_layer(model,name = 'NSP-Dense')$output

outputs = dense %>% layer_dense(units=1L, activation='sigmoid',
                         kernel_initializer=initializer_truncated_normal(stddev = 0.02),
                         name = 'output')

model = keras_model(inputs = inputs,outputs = outputs)

model %>% compile(
  k_bert$AdamWarmup(decay_steps=decay_steps, 
                    warmup_steps=warmup_steps, learning_rate=learning_rate),
  loss = 'binary_crossentropy',
  metrics = 'accuracy'
)

```

```{r}
fit1 <- model %>% fit(
  concat,
  targets,
  epochs=epochs,
  batch_size=bch_size, validation_split=0.2)
```


```{r}
view(x_train)

names(y_train) <- c('user_id.y','coordination_dummy', 'text',  'prob_bot', 'is_retweet', 'followers_count', "favourites_count", "serial_dummy", 'created_at', "quoted_text",             "quoted_created_at",       "quoted_source",           "quoted_favorite_count",  
 "quoted_retweet_count",    "quoted_user_id",          "quoted_screen_name",      "quoted_name",            
"quoted_followers_count",  "quoted_friends_count",    "quoted_statuses_count",   "quoted_location",        
 "quoted_description",      "quoted_verified",         "retweet_status_id",       "retweet_text",           
"retweet_created_at",      "retweet_source",          "retweet_favorite_count",  "retweet_retweet_count",  
 "retweet_user_id",         "retweet_screen_name",     "retweet_name",            "retweet_followers_count",
"retweet_friends_count",   "retweet_statuses_count",  "retweet_location",        "retweet_description",
 "followers_count",         "friends_count",           "listed_count",            "statuses_count",         
 "favourites_count",       "account_created_at",      "verified",                "profile_url")

view(y_train)

```

```{r}

model %>% fit(
  concat[1],
  targets,
  epochs = epochs,
  batch_size=bch_size, validation_split=0.2)
 

```


